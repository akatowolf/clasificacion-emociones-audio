{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cc244a-61c0-400d-a73a-a0b5daab1969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cliente S3 configurado\n   Bucket: amzn-s3-maia-mesd-2026\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "AWS_REGION = \"\" \n",
    "S3_BUCKET = \"\"\n",
    "AWS_SESSION_TOKEN = \"\"\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    aws_session_token=AWS_SESSION_TOKEN,   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc926d5-a281-4bb3-b1ce-cfed607456ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ S3 funciona! Archivos encontrados:\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_abajo.wav\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_adios.wav\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_antes.wav\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_arriba.wav\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_ayer.wav\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_basta_ya.wav\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_de_nada.wav\n  - raw/all-wavs/MexicanEmotionalSpeechDatabase/Anger_C_A_delante.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "S3_PREFIX = \"\"\n",
    "\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=S3_BUCKET,\n",
    "    Prefix=S3_PREFIX,\n",
    "    MaxKeys=10\n",
    ")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    print(f\"Archivos encontrados:\")\n",
    "    for obj in response['Contents']:\n",
    "        if obj['Key'].endswith('.wav'):\n",
    "            print(f\"  - {obj['Key']}\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ce452e-e4ed-41af-a587-e2c1a255d045",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Entrenamiento y funciones"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initiating S3 file listing...\n[INFO] Total audio files located: 862\n[INFO] Class distribution: {'Fear': 144, 'Anger': 143, 'Sadness': 144, 'Neutral': 143, 'Happiness': 144, 'Disgust': 144}\n[INFO] Data split successful. Train subset: 732, Validation subset: 130\n\n[INFO] Extracting features for TRAIN set (Data Augmentation Enabled)...\n[INFO] Processing progress: 50/732 files\n[INFO] Processing progress: 100/732 files\n[INFO] Processing progress: 150/732 files\n[INFO] Processing progress: 200/732 files\n[INFO] Processing progress: 250/732 files\n[INFO] Processing progress: 300/732 files\n[INFO] Processing progress: 350/732 files\n[INFO] Processing progress: 400/732 files\n[INFO] Processing progress: 450/732 files\n[INFO] Processing progress: 500/732 files\n[INFO] Processing progress: 550/732 files\n[INFO] Processing progress: 600/732 files\n[INFO] Processing progress: 650/732 files\n[INFO] Processing progress: 700/732 files\n[INFO] Processing progress: 732/732 files\n\n[INFO] Extracting features for VALIDATION set (Data Augmentation Disabled)...\n[INFO] Processing progress: 50/130 files\n[INFO] Processing progress: 100/130 files\n[INFO] Processing progress: 130/130 files\n\n[INFO] Feature Engineering Completed.\n[INFO] X_train shape: (3660, 288)\n[INFO] X_val shape:   (130, 288)\n[INFO] Detected Classes: [np.str_('Anger'), np.str_('Disgust'), np.str_('Fear'), np.str_('Happiness'), np.str_('Neutral'), np.str_('Sadness')]\n\n------------------------------------------------------------\n[INFO] Training Model: Support Vector Machine (RBF)\n------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/02/21 17:58:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SVM Training Completed.\n[INFO] Train F1 (Macro): 0.8636 | Val F1 (Macro): 0.7870\n\n------------------------------------------------------------\n[INFO] Training Model: Random Forest (Highly Regularized)\n------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/02/21 17:59:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random Forest Training Completed.\n[INFO] Train F1 (Macro): 0.7167 | Val F1 (Macro): 0.6469\n\n------------------------------------------------------------\n[INFO] Training Model: XGBoost (Highly Regularized)\n------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/02/21 17:59:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] XGBoost Training Completed.\n[INFO] Train F1 (Macro): 0.7386 | Val F1 (Macro): 0.5932\n\n============================================================\n[INFO] PIPELINE EXECUTION COMPLETED\n============================================================\n[INFO] Evaluated Models:\n       - Support Vector Machine (Baseline)\n       - Random Forest (Highly Regularized)\n       - XGBoost (Highly Regularized)\n[INFO] Detailed metrics and artifacts are available in the MLflow UI.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random\n",
    "from collections import Counter\n",
    "import io\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress librosa and sklearn warnings for cleaner console output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# MLFLOW CONFIGURATION\n",
    "# ============================================================================\n",
    "EXPERIMENT_NAME = \"/Users/tique.yessicaadriana@gmail.com/emotion-s3-improved\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# NOTE: Ensure s3_client, S3_BUCKET, and S3_PREFIX are properly initialized \n",
    "# before executing the pipeline.\n",
    "\n",
    "# ============================================================================\n",
    "# S3 UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def load_audio_from_s3(bucket, key, sr=16000):\n",
    "    \"\"\"Fetches an audio file from S3 and loads it as a numpy array.\"\"\"\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    audio_bytes = obj['Body'].read()\n",
    "    audio, orig_sr = sf.read(io.BytesIO(audio_bytes), dtype='float32')\n",
    "    \n",
    "    # Convert stereo to mono if necessary\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "        \n",
    "    # Resample audio to target sample rate\n",
    "    if sr is not None and orig_sr != sr:\n",
    "        audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=sr)\n",
    "        \n",
    "    return audio\n",
    "\n",
    "def list_all_wav_files(bucket, prefix=\"\"):\n",
    "    \"\"\"Retrieves all .wav file keys from a given S3 bucket and prefix.\"\"\"\n",
    "    wav_files = []\n",
    "    continuation_token = None\n",
    "    \n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if continuation_token:\n",
    "            kwargs[\"ContinuationToken\"] = continuation_token\n",
    "            \n",
    "        response = s3_client.list_objects_v2(**kwargs)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                if obj['Key'].endswith('.wav'):\n",
    "                    wav_files.append(obj)\n",
    "\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return wav_files\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING & AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "def augment_audio(audio, sr=16000):\n",
    "    \"\"\"\n",
    "    Applies data augmentation to the input audio.\n",
    "    Returns 5 versions: original, pitch shifted (+/-2 steps), and time stretched (+/-10%).\n",
    "    \"\"\"\n",
    "    versions = [\n",
    "        audio,\n",
    "        librosa.effects.pitch_shift(audio, sr=sr, n_steps=2),\n",
    "        librosa.effects.pitch_shift(audio, sr=sr, n_steps=-2),\n",
    "        librosa.effects.time_stretch(audio, rate=1.1),\n",
    "        librosa.effects.time_stretch(audio, rate=0.9)\n",
    "    ]\n",
    "    return versions\n",
    "\n",
    "def extract_features_audio(y, sr=16000):\n",
    "    \"\"\"\n",
    "    Extracts a 288-dimensional feature vector from the audio signal.\n",
    "    Includes MFCCs, Chroma, Spectral features, ZCR, RMS, and Pitch tracking.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Trim silence\n",
    "        y, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "        # Pad audio if it's shorter than the minimum duration (1.0 second)\n",
    "        MIN_DURATION = 1.0\n",
    "        if len(y) < int(MIN_DURATION * sr):\n",
    "            y = np.pad(y, (0, int(MIN_DURATION * sr) - len(y)), mode=\"constant\")\n",
    "\n",
    "        # Peak normalization\n",
    "        if np.max(np.abs(y)) > 0:\n",
    "            y = y / np.max(np.abs(y))\n",
    "\n",
    "        features = []\n",
    "        N_MFCC = 20\n",
    "        HOP_LENGTH = 512\n",
    "        N_FFT = 2048\n",
    "\n",
    "        # 1. MFCCs and Deltas\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "        for M in [mfcc, mfcc_delta, mfcc_delta2]:\n",
    "            features.extend(np.mean(M, axis=1))\n",
    "            features.extend(np.std(M, axis=1))\n",
    "            features.extend(np.min(M, axis=1))\n",
    "            features.extend(np.max(M, axis=1))\n",
    "\n",
    "        # 2. Chroma STFT\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "        features.extend(np.mean(chroma, axis=1))\n",
    "        features.extend(np.std(chroma, axis=1))\n",
    "\n",
    "        # 3. Spectral Features\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "        bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "\n",
    "        for feat in [centroid, bandwidth, rolloff]:\n",
    "            features.extend([float(np.mean(feat)), float(np.std(feat)), float(np.min(feat)), float(np.max(feat))])\n",
    "\n",
    "        # 4. Zero Crossing Rate (ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y, hop_length=HOP_LENGTH)\n",
    "        features.extend([float(np.mean(zcr)), float(np.std(zcr)), float(np.min(zcr)), float(np.max(zcr))])\n",
    "\n",
    "        # 5. Root Mean Square (RMS) Energy\n",
    "        rms = librosa.feature.rms(y=y, hop_length=HOP_LENGTH)\n",
    "        features.extend([float(np.mean(rms)), float(np.std(rms)), float(np.min(rms)), float(np.max(rms))])\n",
    "\n",
    "        # 6. Fundamental Frequency (Pitch)\n",
    "        try:\n",
    "            f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), sr=sr)\n",
    "            f0_clean = f0[~np.isnan(f0)]\n",
    "            if len(f0_clean) > 0:\n",
    "                features.extend([float(np.mean(f0_clean)), float(np.std(f0_clean)), \n",
    "                                 float(np.min(f0_clean)), float(np.max(f0_clean))])\n",
    "            else:\n",
    "                features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "        except Exception:\n",
    "            features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Feature extraction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "def key_to_label(key: str) -> str:\n",
    "    \"\"\"Extracts the emotion label from the S3 file naming convention.\"\"\"\n",
    "    filename = key.split(\"/\")[-1]\n",
    "    return filename.split(\"_\")[0]\n",
    "\n",
    "def build_xy_from_keys(bucket, keys, use_aug=False, sr=16000):\n",
    "    \"\"\"Processes audio files and builds the feature matrix (X) and labels (y).\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    total_keys = len(keys)\n",
    "    \n",
    "    for i, key in enumerate(keys):\n",
    "        try:\n",
    "            audio = load_audio_from_s3(bucket, key, sr=sr)\n",
    "            emotion = key_to_label(key)\n",
    "\n",
    "            # Apply augmentation only if specified (typically for training set)\n",
    "            audios = augment_audio(audio, sr=sr) if use_aug else [audio]\n",
    "            \n",
    "            for a in audios:\n",
    "                feat = extract_features_audio(a, sr=sr)\n",
    "                if feat is not None:\n",
    "                    X_list.append(feat)\n",
    "                    y_list.append(emotion)\n",
    "\n",
    "            # Progress logging\n",
    "            if (i + 1) % 50 == 0 or (i + 1) == total_keys:\n",
    "                print(f\"[INFO] Processing progress: {i+1}/{total_keys} files\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Skipping file {key} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return np.array(X_list), y_list\n",
    "\n",
    "# ============================================================================\n",
    "# MLFLOW LOGGING & EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def log_metrics_block(y_train, y_pred_train, y_val, y_pred_val, le, model_tag=\"model\"):\n",
    "    \"\"\"Computes and logs classification metrics, reports, and confusion matrices to MLflow.\"\"\"\n",
    "    \n",
    "    # Calculate global metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    val_acc   = accuracy_score(y_val, y_pred_val)\n",
    "    train_f1  = f1_score(y_train, y_pred_train, average='macro')\n",
    "    val_f1    = f1_score(y_val, y_pred_val, average='macro')\n",
    "\n",
    "    # Log global metrics\n",
    "    mlflow.log_metric(\"train_acc\", float(train_acc))\n",
    "    mlflow.log_metric(\"val_acc\", float(val_acc))\n",
    "    mlflow.log_metric(\"train_f1_macro\", float(train_f1))\n",
    "    mlflow.log_metric(\"val_f1_macro\", float(val_f1))\n",
    "    mlflow.log_metric(\"gap_f1\", float(train_f1 - val_f1))\n",
    "\n",
    "    # Generate and log classification report artifact\n",
    "    report = classification_report(y_val, y_pred_val, target_names=le.classes_, digits=4, zero_division=0)\n",
    "    report_path = f\"/tmp/{model_tag}_classification_report.txt\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    mlflow.log_artifact(report_path)\n",
    "\n",
    "    # Generate and log confusion matrix artifact\n",
    "    cm = confusion_matrix(y_val, y_pred_val)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    disp.plot(ax=ax, xticks_rotation=45)\n",
    "    plt.tight_layout()\n",
    "    cm_path = f\"/tmp/{model_tag}_confusion_matrix.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close(fig)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "\n",
    "    return train_f1, val_f1\n",
    "\n",
    "# ============================================================================\n",
    "# PIPELINE EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def execute_training_pipeline(max_files=None, test_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Executes the comprehensive ML pipeline:\n",
    "    - Data splitting\n",
    "    - Feature extraction (with data augmentation for training)\n",
    "    - Model training (SVM, Highly Regularized Random Forest, Highly Regularized XGBoost)\n",
    "    - MLflow logging\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1. DATA COLLECTION & SPLITTING\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"[INFO] Initiating S3 file listing...\")\n",
    "    wav_files = list_all_wav_files(S3_BUCKET, S3_PREFIX)\n",
    "    random.shuffle(wav_files)\n",
    "    \n",
    "    if max_files:\n",
    "        wav_files = wav_files[:max_files]\n",
    "\n",
    "    keys = [o[\"Key\"] for o in wav_files]\n",
    "    labels = [key_to_label(k) for k in keys]\n",
    "\n",
    "    print(f\"[INFO] Total audio files located: {len(keys)}\")\n",
    "    print(f\"[INFO] Class distribution: {dict(Counter(labels))}\")\n",
    "\n",
    "    # Encode labels\n",
    "    le_keys = LabelEncoder()\n",
    "    y_keys = le_keys.fit_transform(labels)\n",
    "\n",
    "    # Stratified split to preserve class distribution\n",
    "    train_keys, val_keys, _, _ = train_test_split(\n",
    "        keys, y_keys, test_size=test_size, stratify=y_keys, random_state=random_state\n",
    "    )\n",
    "    print(f\"[INFO] Data split successful. Train subset: {len(train_keys)}, Validation subset: {len(val_keys)}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2. FEATURE EXTRACTION\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n[INFO] Extracting features for TRAIN set (Data Augmentation Enabled)...\")\n",
    "    X_train, y_train_str = build_xy_from_keys(S3_BUCKET, train_keys, use_aug=True, sr=16000)\n",
    "\n",
    "    print(\"\\n[INFO] Extracting features for VALIDATION set (Data Augmentation Disabled)...\")\n",
    "    X_val, y_val_str = build_xy_from_keys(S3_BUCKET, val_keys, use_aug=False, sr=16000)\n",
    "\n",
    "    # Consistent label encoding across splits\n",
    "    le = LabelEncoder()\n",
    "    le.fit(le_keys.classes_)\n",
    "    y_train = le.transform(y_train_str)\n",
    "    y_val   = le.transform(y_val_str)\n",
    "\n",
    "    print(\"\\n[INFO] Feature Engineering Completed.\")\n",
    "    print(f\"[INFO] X_train shape: {X_train.shape}\")\n",
    "    print(f\"[INFO] X_val shape:   {X_val.shape}\")\n",
    "    print(f\"[INFO] Detected Classes: {list(le.classes_)}\")\n",
    "\n",
    "    if len(le.classes_) <= 1:\n",
    "        print(\"[ERROR] Dataset contains only one class. Training aborted.\")\n",
    "        return None\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3. MLFLOW MODEL TRACKING\n",
    "    # ------------------------------------------------------------------------\n",
    "    with mlflow.start_run(run_name=\"Classical_ML_Pipeline\") as parent:\n",
    "        \n",
    "        # Log global pipeline parameters\n",
    "        mlflow.log_params({\n",
    "            \"split_strategy\": \"by_file\",\n",
    "            \"augmentation\": \"train_only_5x\",\n",
    "            \"max_files\": str(max_files),\n",
    "            \"test_size\": test_size,\n",
    "            \"random_state\": random_state,\n",
    "            \"train_samples_augmented\": int(len(X_train)),\n",
    "            \"val_samples_original\": int(len(X_val)),\n",
    "        })\n",
    "\n",
    "        # --- MODEL 1: Support Vector Machine (Baseline) ---\n",
    "        with mlflow.start_run(run_name=\"SVM_RBF\", nested=True) as run:\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"[INFO] Training Model: Support Vector Machine (RBF)\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            svm_pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('pca', PCA(n_components=0.80, random_state=42)),\n",
    "                ('svm', SVC(C=0.676, gamma=0.00240, kernel='rbf', class_weight='balanced', random_state=42))\n",
    "            ])\n",
    "            \n",
    "            mlflow.log_params({\n",
    "                \"model\": \"SVC\",\n",
    "                \"pca_variance_retained\": 0.80,\n",
    "                \"svm_C\": 0.676,\n",
    "                \"svm_gamma\": 0.00240,\n",
    "                \"svm_kernel\": \"rbf\",\n",
    "                \"svm_class_weight\": \"balanced\",\n",
    "            })\n",
    "            \n",
    "            svm_pipeline.fit(X_train, y_train)\n",
    "            train_f1, val_f1 = log_metrics_block(\n",
    "                y_train, svm_pipeline.predict(X_train),\n",
    "                y_val, svm_pipeline.predict(X_val),\n",
    "                le, model_tag=\"SVM_RBF\"\n",
    "            )\n",
    "            mlflow.sklearn.log_model(svm_pipeline, \"model\")\n",
    "            \n",
    "            print(f\"[INFO] SVM Training Completed.\")\n",
    "            print(f\"[INFO] Train F1 (Macro): {train_f1:.4f} | Val F1 (Macro): {val_f1:.4f}\")\n",
    "\n",
    "        # --- MODEL 2: Highly Regularized Random Forest ---\n",
    "        with mlflow.start_run(run_name=\"RF_Highly_Regularized\", nested=True) as run:\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"[INFO] Training Model: Random Forest (Highly Regularized)\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            rf_pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('rf', RandomForestClassifier(\n",
    "                    n_estimators=500,\n",
    "                    max_depth=6,              # Aggressive depth limit\n",
    "                    min_samples_split=60,     # Prevents splitting augmented versions of the same file\n",
    "                    min_samples_leaf=20,      # Enforces generalized leaf nodes\n",
    "                    max_features=0.1,         # Forces feature diversity across trees\n",
    "                    max_samples=0.6,          # Subsampling to prevent overfitting\n",
    "                    bootstrap=True,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                ))\n",
    "            ])\n",
    "            \n",
    "            mlflow.log_params({\n",
    "                \"model\": \"RandomForest_Regularized\",\n",
    "                \"rf_n_estimators\": 500,\n",
    "                \"rf_max_depth\": 6,\n",
    "                \"rf_min_samples_split\": 60,\n",
    "                \"rf_min_samples_leaf\": 20,\n",
    "                \"rf_max_features\": 0.1,\n",
    "                \"rf_max_samples\": 0.6,\n",
    "            })\n",
    "            \n",
    "            rf_pipeline.fit(X_train, y_train)\n",
    "            train_f1, val_f1 = log_metrics_block(\n",
    "                y_train, rf_pipeline.predict(X_train),\n",
    "                y_val, rf_pipeline.predict(X_val),\n",
    "                le, model_tag=\"RF_Regularized\"\n",
    "            )\n",
    "            mlflow.sklearn.log_model(rf_pipeline, \"model\")\n",
    "            \n",
    "            print(f\"[INFO] Random Forest Training Completed.\")\n",
    "            print(f\"[INFO] Train F1 (Macro): {train_f1:.4f} | Val F1 (Macro): {val_f1:.4f}\")\n",
    "\n",
    "        # --- MODEL 3: Highly Regularized XGBoost ---\n",
    "        try:\n",
    "            from xgboost import XGBClassifier\n",
    "            \n",
    "            with mlflow.start_run(run_name=\"XGBoost_Highly_Regularized\", nested=True) as run:\n",
    "                print(\"\\n\" + \"-\"*60)\n",
    "                print(\"[INFO] Training Model: XGBoost (Highly Regularized)\")\n",
    "                print(\"-\"*60)\n",
    "                \n",
    "                xgb_pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=0.75, random_state=42)),\n",
    "                    ('xgb', XGBClassifier(\n",
    "                        n_estimators=200,          \n",
    "                        learning_rate=0.03,        # Slower learning rate for stability\n",
    "                        max_depth=3,               # Reduced to shallow trees (stumps)\n",
    "                        min_child_weight=20,       # High minimum weight to prevent specific splits\n",
    "                        subsample=0.6,             \n",
    "                        colsample_bytree=0.5,      \n",
    "                        gamma=3.0,                 # High penalty for new node creation\n",
    "                        reg_alpha=2.0,             # L1 regularization\n",
    "                        reg_lambda=10.0,           # L2 regularization\n",
    "                        objective=\"multi:softprob\",\n",
    "                        eval_metric=\"mlogloss\",\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1\n",
    "                    ))\n",
    "                ])\n",
    "                \n",
    "                mlflow.log_params({\n",
    "                    \"model\": \"XGBoost_Regularized\",\n",
    "                    \"xgb_learning_rate\": 0.03,\n",
    "                    \"xgb_max_depth\": 3,\n",
    "                    \"xgb_min_child_weight\": 20,\n",
    "                    \"xgb_subsample\": 0.6,\n",
    "                    \"xgb_colsample_bytree\": 0.5,\n",
    "                    \"xgb_gamma\": 3.0,\n",
    "                    \"xgb_reg_alpha\": 2.0,\n",
    "                    \"xgb_reg_lambda\": 10.0,\n",
    "                })\n",
    "                \n",
    "                xgb_pipeline.fit(X_train, y_train)\n",
    "                train_f1, val_f1 = log_metrics_block(\n",
    "                    y_train, xgb_pipeline.predict(X_train),\n",
    "                    y_val, xgb_pipeline.predict(X_val),\n",
    "                    le, model_tag=\"XGBoost_Regularized\"\n",
    "                )\n",
    "                mlflow.sklearn.log_model(xgb_pipeline, \"model\")\n",
    "                \n",
    "                print(f\"[INFO] XGBoost Training Completed.\")\n",
    "                print(f\"[INFO] Train F1 (Macro): {train_f1:.4f} | Val F1 (Macro): {val_f1:.4f}\")\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"\\n[WARNING] XGBoost library not found. Skipping XGBoost model training.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] XGBoost training failed: {e}\")\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # 4. SUMMARY\n",
    "        # --------------------------------------------------------------------\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"[INFO] PIPELINE EXECUTION COMPLETED\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"[INFO] Evaluated Models:\")\n",
    "        print(\"       - Support Vector Machine (Baseline)\")\n",
    "        print(\"       - Random Forest (Highly Regularized)\")\n",
    "        print(\"       - XGBoost (Highly Regularized)\")\n",
    "        print(\"[INFO] Detailed metrics and artifacts are available in the MLflow UI.\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# ============================================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure to configure S3_BUCKET and S3_PREFIX globally before execution\n",
    "    execute_training_pipeline(max_files=None)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Modelos clásicos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}